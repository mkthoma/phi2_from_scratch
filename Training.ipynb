{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0606b249-c538-428c-b585-cf97eb2fa9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'phi2_from_scratch'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 30 (delta 0), reused 30 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (30/30), 756.53 KiB | 13.75 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mkthoma/phi2_from_scratch.git\n",
    "!pip install --quiet --no-cache-dir --ignore-installed -r phi2_from_scratch/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fe25df-afc2-467f-a140-580450f5f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'phi-2', 'name': 'phi2_training', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.0001, 'batch_size': 4, 'micro_batch_size': 2, 'gradient_accumulation_steps': 2, 'max_iters': 15000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 15000, 'min_lr': 1e-08}\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank: 0] Seed set to 1337\n",
      "Loading model with {'name': 'phi-2', 'hf_config': {'org': 'microsoft', 'name': 'phi-2'}, 'block_size': 2048, 'vocab_size': 50257, 'padding_multiple': 512, 'padded_vocab_size': 51200, 'n_layer': 32, 'n_head': 32, 'n_embd': 2560, 'rotary_percentage': 0.4, 'parallel_residual': True, 'bias': True, 'lm_head_bias': True, 'n_query_groups': 32, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'tanh', 'intermediate_size': 10240, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 80, 'rope_n_elem': 32}\n",
      "[rank: 1] Seed set to 1337\n",
      "Time to instantiate model: 0.12 seconds.\n",
      "Total parameters 2,779,683,840\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/phi2_from_scratch/phi2_training.py\", line 331, in <module>\n",
      "    setup(\n",
      "  File \"/home/ubuntu/phi2_from_scratch/phi2_training.py\", line 77, in setup\n",
      "    fabric.launch(main, train_data_dir, val_data_dir, resume)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 839, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 924, in _wrap_and_launch\n",
      "    return launcher.launch(to_run, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/strategies/launchers/subprocess_script.py\", line 104, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 930, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/phi2_from_scratch/phi2_training.py\", line 127, in main\n",
      "    model = fabric.setup(model)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 243, in setup\n",
      "    module = self._strategy.setup_module(module)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/strategies/fsdp.py\", line 288, in setup_module\n",
      "    module = FullyShardedDataParallel(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 463, in __init__\n",
      "    _auto_wrap(\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_wrap_utils.py\", line 72, in _auto_wrap\n",
      "    _post_order_apply(root_module, wrap_fn)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 79, in _post_order_apply\n",
      "    _post_order_apply_inner(root_module, \"\", None)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 63, in _post_order_apply_inner\n",
      "    _post_order_apply_inner(child_module, child_module_name, module)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 63, in _post_order_apply_inner\n",
      "    _post_order_apply_inner(child_module, child_module_name, module)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 63, in _post_order_apply_inner\n",
      "    _post_order_apply_inner(child_module, child_module_name, module)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 64, in _post_order_apply_inner\n",
      "    optional_module = fn(module)\n",
      "                      ^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 98, in fn\n",
      "    return fsdp_fn(module, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 487, in __init__\n",
      "    _init_param_handle_from_module(\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py\", line 498, in _init_param_handle_from_module\n",
      "    _materialize_meta_module(fully_sharded_module, device_id)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py\", line 809, in _materialize_meta_module\n",
      "    raise e\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py\", line 801, in _materialize_meta_module\n",
      "    module.to_empty(device=materialization_device, recurse=False)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1039, in to_empty\n",
      "    return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1039, in <lambda>\n",
      "    return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/_refs/__init__.py\", line 4681, in empty_like\n",
      "    return torch.empty_permuted(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/phi2_from_scratch/phi2_training.py\", line 331, in <module>\n",
      "    setup(\n",
      "  File \"/home/ubuntu/phi2_from_scratch/phi2_training.py\", line 77, in setup\n",
      "    fabric.launch(main, train_data_dir, val_data_dir, resume)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 839, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 925, in _wrap_and_launch\n",
      "    return to_run(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 930, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/phi2_from_scratch/phi2_training.py\", line 127, in main\n",
      "    model = fabric.setup(model)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/fabric.py\", line 249, in setup\n",
      "    _update_properties(\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 115, in _update_properties\n",
      "    root.apply(apply_fn)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 897, in apply\n",
      "    module.apply(fn)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 584, in apply\n",
      "    with _unshard_params_recurse(\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/contextlib.py\", line 137, in __enter__\n",
      "    return next(self.gen)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_unshard_param_utils.py\", line 281, in _unshard_params_recurse\n",
      "    with _unshard_fsdp_state_params(\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/contextlib.py\", line 137, in __enter__\n",
      "    return next(self.gen)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_unshard_param_utils.py\", line 196, in _unshard_fsdp_state_params\n",
      "    _unshard(state, handle, computation_stream, computation_stream)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py\", line 347, in _unshard\n",
      "    handle.unshard()\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/flat_param.py\", line 1251, in unshard\n",
      "    padded_unsharded_flat_param = self._all_gather_flat_param(unsharded_flat_param)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/fsdp/flat_param.py\", line 1339, in _all_gather_flat_param\n",
      "    dist.all_gather_into_tensor(\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 2897, in all_gather_into_tensor\n",
      "    work = group._allgather_base(output_tensor, input_tensor)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer. This may indicate a possible application crash on rank 0 or a network set up issue.\n"
     ]
    }
   ],
   "source": [
    "!python phi2_from_scratch/phi2_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210518c-dc65-4933-afcc-3e877d95578a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
